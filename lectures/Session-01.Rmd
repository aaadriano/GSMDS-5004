---
title: "Outline, Review and Ethics"
author: "Jameson Watts, Ph.D."
date: "1/16/2020"
output: 
  ioslides_presentation:
    smaller: yes
    widescreen: yes
    df_print: kable
---
<style>
strong{
  color: #018080;
}
table.rmdtable th {
    background: #791716;
}

</style>

## Agenda

1. Course Overview
2. Review of Multiple Regression
3. Ethics of data in math
4. Ethics of data in policy 

## But first...

Rescheduling Saturday the 25th?

## Setup

```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
source('theme.R')
wine = read_rds("../resources/wine.rds")
```

# Course Overview

## Expectations and assignments

1. Data Camp Assignments
2. R vs. Python
3. Exams
4. Modeling Project

<!-- ## Marriage of Python and R -->
<!-- ```{r} -->
<!-- library(reticulate) -->
<!-- path_to_python <- "/opt/anaconda3/bin/python" -->
<!-- use_python(path_to_python) -->
<!-- ``` -->


# Review of Multiple Regression

## Basic model

```{r}
library(moderndive)
wine <- wine %>% mutate(bordeaux=(province=="Bordeaux"))
get_regression_table(lm(price ~ points, data = wine))
```

## Multiple regression
```{r}
get_regression_table(lm(price ~ points+bordeaux, data = wine))
```

## Model diagnostics on full data set
```{r}
get_regression_summaries(lm(price ~ points, data = wine))
```


```{r}
get_regression_summaries(lm(price ~ points+bordeaux, data = wine))
```

## Split sample using Caret
```{r}
library(caret)
set.seed(5004) #for reproducibility
train_index <- createDataPartition(wine$price, times = 1, p = 0.8, list = FALSE)
train <- wine[train_index, ]
test <- wine[-train_index, ]

m1 <- lm(price~points, data = train)
m2 <- lm(price~points+bordeaux, data = train)

```

## Comparing RMSE

```{r}
get_regression_points(m1, newdata = test) %>% 
  drop_na(residual) %>% 
  mutate(sq_residuals = residual^2) %>% 
  summarize(rmse = sqrt(mean(sq_residuals)))

get_regression_points(m2, newdata = test) %>% 
  drop_na(residual) %>% 
  mutate(sq_residuals = residual^2) %>% 
  summarize(rmse = sqrt(mean(sq_residuals)))

```


## What about an interaction?

```{r}
m3 <- lm(price~points*bordeaux, data = train)
get_regression_table(m3)

get_regression_points(m3, newdata = test) %>% 
  drop_na(residual) %>% 
  mutate(sq_residuals = residual^2) %>% 
  summarize(rmse = sqrt(mean(sq_residuals)))
```

# So what is machine learning?

## Next steps...

Definition: using data to find a function that minimizes prediction error.

- Feature Engineering
- Variable Selection
- Cross validation
- Classification
  + Confusion matrix
  + ROC curves


# Ethics of data 

## The math of it...

Suppose I'm trying to predict gender based on height. We start by defining the outcome and predictors and creating training and test data.

```{r}
library(dslabs)
data(heights)
y <- heights$sex
x <- heights$height
set.seed(5004)
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
test_set <- heights[test_index, ]
train_set <- heights[-test_index, ]
```

Note: this vignette is adapted from [this book](https://rafalab.github.io/dsbook/introduction-to-machine-learning.html)

## Guessing.

Letâ€™s start by developing the simplest possible machine algorithm: guessing the outcome.
```{r}
y_hat <- sample(c("Male", "Female"), length(test_index), replace = TRUE) %>%
  factor(levels = levels(test_set$sex))
```

The overall accuracy is simply defined as the overall proportion that is predicted correctly:
```{r}
mean(y_hat == test_set$sex)
```

## Let's do better...

```{r}
heights %>% group_by(sex) %>% summarize(mean(height), sd(height))
```

Predict male if within 2 standard deviations

```{r}
y_hat <- ifelse(x > 62, "Male", "Female") %>% 
  factor(levels = levels(test_set$sex))

mean(y == y_hat)
```

The accuracy goes up from 0.50 to about 0.80!!

## Let's optimize

```{r}
cutoff <- seq(61, 70)
accuracy <- map_dbl(cutoff, function(x){
  y_hat <- ifelse(train_set$height > x, "Male", "Female") %>% 
    factor(levels = levels(test_set$sex))
  mean(y_hat == train_set$sex)
})

max(accuracy)
```

which is much higher than 0.5. The cutoff resulting in this accuracy is:

```{r}
best_cutoff <- cutoff[which.max(accuracy)]
best_cutoff
```

## How does it do on the test data?

```{r}
y_hat <- ifelse(test_set$height > best_cutoff, "Male", "Female") %>% 
  factor(levels = levels(test_set$sex))
y_hat <- factor(y_hat)
mean(y_hat == test_set$sex)
```

Not quite as good as the training set, but pretty good nonetheless. 

...but does this make sense?

## Confusion matrix

```{r}
table(predicted = y_hat, actual = test_set$sex)
```

what do you see?

## Accuracy by sex

```{r}
test_set %>% 
  mutate(y_hat = y_hat) %>%
  group_by(sex) %>% 
  summarize(accuracy = mean(y_hat == sex))
```


There is an imbalance in the force! We are literally calling almost half of the females male! 

So why is the overall accuracy so high then? 

## Moral of the story

...too many men.

## Other ethical issues

- Demographic data
- Profit optimizing
- Autonomous cars
- Recommendation engines
- Criminal sentencing
- Choice of classification model
- Killer robots


Reasonable people will disagree over subtle matters of right and wrong... thus, the important part of data ethics is committing to *consider* the ethical consequences of your choices. 

The difference between "regular" ethics and data ethics is that algorithms scale really easily. Thus, seemingly small decisions can have wide-ranging impact.

# Ethics policy and technology

with my friend Jeff Gaus.
